{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p float=\"center\">\n",
    "  <img src=\"images/horizontal.png\" alt=\"Coiled logo\" width=\"415\" hspace=\"10\"/>\n",
    "  <img src=\"images/dask_horizontal_no_pad.svg\" alt=\"Dask logo\" width=\"200\" hspace=\"10\" />\n",
    "</p>\n",
    "\n",
    "# Parallel and Distributed Machine Learning\n",
    "\n",
    "*The material in this notebook riffs off of the open-source content from [Dask's tutorial repository](https://github.com/dask/dask-tutorial).*\n",
    "\n",
    "We've now seen how Dask makes data analysis scalable with parallelization via Dask DataFrames. Let's now see how [Dask-ML](https://dask-ml.readthedocs.io) allows us to do machine learning in a parallel and distributed manner. Note, machine learning is really just a special case of data analysis (one that automates analytical model building), so the ðŸ’ª Dask gains ðŸ’ª we've seen will apply here as well!\n",
    "\n",
    "(If you'd like a refresher on the difference between parallel and distributed computing, [here's a good discussion on StackExchange](https://cs.stackexchange.com/questions/1580/distributed-vs-parallel-computing).)\n",
    "\n",
    "In this notebook, we'll \n",
    "\n",
    "* break down machine learning scaling problems into two categories.\n",
    "* review how Scikit-Learn works.\n",
    "* solve an ML problem with a single machine (with Scikit-Learn).\n",
    "* solve an ML problem with a single machine and parallelism (with Scikit-Learn and Joblib).\n",
    "* solve an ML problem with a multiple machines and parallelism (with Scikit-Learn, Joblib and Dask).\n",
    "* solve an ML problem with a multiple machines *in the cloud* and parallelism (with Scikit-Learn, Joblib, Dask and Coiled).\n",
    "\n",
    "*A bit about me:* I'm Hugo Bowne-Anderson, Head of Data Science Evangelism and Marketing at [Coiled](coiled.io/). We build products that bring the power of scalable data science and machine learning to you, such as single-click hosted clusters on the cloud. We want to take the DevOps out of data science so you can get back to your real job. If you're interested in taking Coiled for a test drive, you can sign up for our [free Beta here](beta.coiled.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Types of scaling problems in machine learning\n",
    "\n",
    "So you have your machine learning workflow that works well for small problems. Then there are two main types of scaling challenges you can run into: scaling the **size of your data** and scaling the **size of your model**. That is:\n",
    "\n",
    "1. **CPU-bound problems**: Data fits in RAM, but training takes too long. Many hyperparameter combinations, a large ensemble of many models, etc.\n",
    "2. **Memory-bound problems**: Data is larger than RAM, and sampling isn't an option.\n",
    "\n",
    "Here's a handy diagram for visualizing these problems:\n",
    "\n",
    "![](images/ml-dimensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bottom-left quadrant, your datasets arenâ€™t too large (and therefore fit comfortably in RAM) and your model isnâ€™t too large. Here, youâ€™re much better off using something like scikit-learn, XGBoost, and similar libraries. You don't need to leverage multiple machines in a distributed manner with a library like Dask-ML here.\n",
    "\n",
    "If youâ€™re in any of the other quadrants, however, distributed machine learning is the way to go.\n",
    "\n",
    "Here's a bird's eye view of the strategy we'll apply in this notebook:\n",
    "\n",
    "* For in-memory problems, just use scikit-learn (or your favorite ML library).\n",
    "* For large models, use `dask_ml.joblib` and your favorite scikit-learn estimator.\n",
    "* For large datasets, use `dask_ml` estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scikit-Learn in five minutes\n",
    "\n",
    "<img src=\"images/scikit_learn_logo_small.svg\" alt=\"scikit-learn logo\"/>\n",
    "\n",
    "\n",
    "In this section, we'll quickly run through a typical Scikit-Learn workflow:\n",
    "\n",
    "* Load some data (in this case, we'll generate it)\n",
    "* Import the Scikit-Learn module for our chosen ML algorithm\n",
    "* Create an estimator for that algorithm and fit it with our data\n",
    "* Inspect the learned attributes\n",
    "* Check the accurary of our model\n",
    "\n",
    "Scikit-Learn has a nice, consistent API:\n",
    "\n",
    "* You instantiate an `Estimator` (e.g. `LinearRegression`, `RandomForestClassifier`, etc.). All of the models *hyperparameters* (user-specified parameters, not the ones learned by the estimator) are passed to the estimator when it's created.\n",
    "* You call `estimator.fit(X, y)` to train the estimator.\n",
    "* Use `estimator` to inspect attributes, make predictions, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the workflow. Let's first generate some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77244139,  0.3607576 , -2.38110133,  0.08757   ],\n",
       "       [ 1.14946035,  0.62254594,  0.37302939,  0.45965795],\n",
       "       [-1.90879217, -1.1602627 , -0.27364545, -0.82766028],\n",
       "       [-0.77694695,  0.31434299, -2.26231851,  0.06339125],\n",
       "       [-1.17047054,  0.02212382, -2.17376797, -0.13421976],\n",
       "       [ 0.79010037,  0.68530624, -0.44740487,  0.44692959],\n",
       "       [ 1.68616989,  1.6329131 , -1.42072654,  1.04050557],\n",
       "       [-0.93912893, -1.02270838,  1.10093827, -0.63714432]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=4, random_state=0)\n",
    "X[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit a Support Vector Classifier for this example, so let's load the appropriate Scikit-Learn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the estimator and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SVC(random_state=0)\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect the learned attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77244139,  0.3607576 , -2.38110133,  0.08757   ],\n",
       "       [ 1.14946035,  0.62254594,  0.37302939,  0.45965795],\n",
       "       [-0.77694695,  0.31434299, -2.26231851,  0.06339125],\n",
       "       [ 0.79010037,  0.68530624, -0.44740487,  0.44692959]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models have *hyperparameters*. They affect the fit, but are specified up front instead of learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77244139,  0.3607576 , -2.38110133,  0.08757   ],\n",
       "       [ 1.14946035,  0.62254594,  0.37302939,  0.45965795],\n",
       "       [-0.77694695,  0.31434299, -2.26231851,  0.06339125],\n",
       "       [-1.17047054,  0.02212382, -2.17376797, -0.13421976]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = SVC(C=0.00001, shrinking=False, random_state=0)\n",
    "estimator.fit(X, y)\n",
    "estimator.support_vectors_[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5007"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Optimization\n",
    "\n",
    "There are a few ways to learn the best *hyper*parameters while training. One is `GridSearchCV`.\n",
    "As the name implies, this does a brute-force search over a grid of hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] C=0.001, kernel=rbf .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................. C=0.001, kernel=rbf, total=   3.0s\n",
      "[CV] C=0.001, kernel=rbf .............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................. C=0.001, kernel=rbf, total=   3.1s\n",
      "[CV] C=0.001, kernel=poly ............................................\n",
      "[CV] ............................. C=0.001, kernel=poly, total=   1.5s\n",
      "[CV] C=0.001, kernel=poly ............................................\n",
      "[CV] ............................. C=0.001, kernel=poly, total=   1.5s\n",
      "[CV] C=10.0, kernel=rbf ..............................................\n",
      "[CV] ............................... C=10.0, kernel=rbf, total=   1.0s\n",
      "[CV] C=10.0, kernel=rbf ..............................................\n",
      "[CV] ............................... C=10.0, kernel=rbf, total=   1.0s\n",
      "[CV] C=10.0, kernel=poly .............................................\n",
      "[CV] .............................. C=10.0, kernel=poly, total=   2.0s\n",
      "[CV] C=10.0, kernel=poly .............................................\n",
      "[CV] .............................. C=10.0, kernel=poly, total=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   15.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 385 ms, total: 18.6 s\n",
      "Wall time: 18.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=SVC(gamma='auto', probability=True, random_state=0),\n",
       "             param_grid={'C': [0.001, 10.0], 'kernel': ['rbf', 'poly']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "estimator = SVC(gamma='auto', random_state=0, probability=True)\n",
    "param_grid = {\n",
    "    'C': [0.001, 10.0],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:** We have\n",
    "\n",
    "* broke ML scaling problems into two categories.\n",
    "  * CPU-bound problems where scaling up the model size is the issue\n",
    "  * RAM-bound problems where scaling up the data size is the issue\n",
    "* carried out a typical Scikit-Learn workflow for ML problems with small model(s) and small data.\n",
    "* reviewed hyperparameters and hyperparameter optimization in Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single-machine parallelism with Joblib\n",
    "\n",
    "<img src=\"images/joblib_logo.svg\" alt=\"Joblib logo\" style=\"width: 300px;\"/>\n",
    "\n",
    "In this section we'll see how [Joblib](https://joblib.readthedocs.io/en/latest/) (\"*a set of tools to provide lightweight pipelining in Python*\") gives us parallelism on our laptop. Here's what our grid search graph would look like if we set up six training \"jobs\" in parallel:\n",
    "\n",
    "![](images/unmerged_grid_search_graph.svg)\n",
    "\n",
    "With Joblib, we can say that Scikit-Learn has *single-machine* parallelism.\n",
    "Any Scikit-Learn estimator that can operate in parallel exposes an `n_jobs` keyword.\n",
    "This controls the number of CPU cores that will be used. Specifying `-1` jobs means using all processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    8.4s remaining:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   11.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   11.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.71 s, sys: 275 ms, total: 3.98 s\n",
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=SVC(gamma='auto', probability=True, random_state=0),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': [0.001, 10.0], 'kernel': ['rbf', 'poly']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=2, n_jobs=-1)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:** Previously, we\n",
    "\n",
    "* solved an ML problem with a single machine (with Scikit-Learn).\n",
    "\n",
    "In this section, we\n",
    "\n",
    "* solved an ML problem with a single machine and parallelism (with Scikit-Learn and Joblib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-machine parallelism with Dask\n",
    "\n",
    "<img src=\"images/dask_horizontal_no_pad.svg\" alt=\"Dask logo\" style=\"width: 500px;\"/>\n",
    "\n",
    "In this section we'll see how Dask (plus Joblib and Scikit-Learn) gives us multi-machine parallelism. Here's what our grid search graph would look like if we allowed Dask to schedule our training \"jobs\" over multiple machines in our cluster:\n",
    "\n",
    "![](images/merged_grid_search_graph.svg)\n",
    "\n",
    "We can say that Dask can talk to Scikit-Learn (via Joblib) so that our *cluster* is used to train a model. \n",
    "\n",
    "If we run this on a laptop, it will take quite some time, but the CPU usage will be satisfyingly near 100% for the duration. To run faster, we would need a distributed cluster. That would mean putting something in the call to `Client` something like\n",
    "\n",
    "```\n",
    "c = Client('tcp://my.scheduler.address:8786')\n",
    "```\n",
    "\n",
    "Details on the many ways to create a cluster can be found [here](https://docs.dask.org/en/latest/setup/single-distributed.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a larger problem (more hyperparameters). First we'll instantiante a Dask Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hugobowne-anderson/opt/anaconda3/envs/data-science-at-scale/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 65243 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import dask.distributed\n",
    "\n",
    "c = dask.distributed.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we expand our problem by specifying more hyperparameters before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.1, 1.0, 2.5, 5, 10.0],\n",
    "    # Uncomment this for larger Grid searches on a cluster\n",
    "    # 'kernel': ['rbf', 'poly', 'linear'],\n",
    "    # 'shrinking': [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid, verbose=2, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, watch this. We can fit our estimator with multi-machine parallelism by quickly *switching to a Dask parallel backend*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 s, sys: 1.46 s, total: 17.3 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(\"dask\", scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this work so seamlessly? Dask-ML developers worked with the Scikit-Learn and Joblib developers to implement a Dask parallel backend. So internally, scikit-learn now talks to Joblib, and Joblib talks to Dask, and Dask is what handles scheduling all of those tasks on multiple machines.\n",
    "\n",
    "The best parameters and best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 10.0}, 0.9119000000000002)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:** Previously, we\n",
    "\n",
    "* solved an ML problem with a single machine (with Scikit-Learn).\n",
    "* solved an ML problem with a single machine and parallelism (with Scikit-Learn and Joblib).\n",
    "\n",
    "In this section, we\n",
    "\n",
    "* solved an ML problem with a multiple machines and parallelism (with Scikit-Learn, Joblib and Dask)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-machine parallelism in the cloud with Coiled\n",
    "\n",
    "<br>\n",
    "<img src=\"images/horizontal.png\" alt=\"Coiled logo\" style=\"width: 500px;\"/>\n",
    "<br>\n",
    "\n",
    "In this section we'll see how Coiled allows us to solve machine learning problems with multi-machine parallelism in the cloud.\n",
    "\n",
    "Coiled, [among other things](https://coiled.io/why-coiled/), provides hosted and scalable Dask clusters. The biggest barriers to entry for doing machine learning at scale are \"Do you have access to a cluster?\" and \"Do you know how to manage it?\" Coiled solves both of those problems. Let's see how.\n",
    "\n",
    "We'll spin up a Coiled cluster (with 10 workers in this case), then instantiante a Dask Client to use with that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "from dask.distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Cluster. This takes about a minute ...Checking environment images\n",
      "Valid environment image found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tls://ec2-18-217-2-109.us-east-2.compute.amazonaws.com:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://ec2-18-217-2-109.us-east-2.compute.amazonaws.com:8787/status' target='_blank'>http://ec2-18-217-2-109.us-east-2.compute.amazonaws.com:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tls://10.1.12.74:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spin up cluster, instantiate a Client\n",
    "cluster = coiled.Cluster(n_workers=10, configuration=\"my-cluster-config\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can fit our estimator with multi-machine paralellism by quickly switching to a Dask parallel backend. This time, this multi-machine parallelism is *in the cloud* because we've set up our Dask clusters via Coiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  30 | elapsed:    8.7s remaining:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:   23.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.06 s, sys: 820 ms, total: 8.88 s\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with joblib.parallel_backend(\"dask\", scatter=[X, y]):\n",
    "    grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cluster being a cloud-based cluster that adds no complexity is Coiled's mission on full display.\n",
    "\n",
    "The best parameters and best score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'C': 10.0}, 0.9119000000000002)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap:** Previously, we\n",
    "\n",
    "* solved an ML problem with a single machine (with Scikit-Learn).\n",
    "* solved an ML problem with a single machine and parallelism (with Scikit-Learn and Joblib).\n",
    "* solved an ML problem with a multiple machines and parallelism (with Scikit-Learn, Joblib and Dask).\n",
    "\n",
    "In this section, we\n",
    "\n",
    "* solved an ML problem with a multiple machines *in the cloud* and parallelism (with Scikit-Learn, Joblib, Dask and Coiled)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus! Training on large datasets\n",
    "\n",
    "Let's talk about one more thing. Sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on Dask Arrays and DataFrames that may be larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.delayed\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a small (random) dataset locally using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centers = 12\n",
    "n_features = 20\n",
    "\n",
    "X_small, y_small = make_blobs(n_samples=1000, centers=n_centers, n_features=n_features, random_state=0)\n",
    "\n",
    "centers = np.zeros((n_centers, n_features))\n",
    "\n",
    "for i in range(n_centers):\n",
    "    centers[i] = X_small[y_small == i].mean(0)\n",
    "    \n",
    "centers[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small dataset will be the template for our large random dataset.\n",
    "We'll use `dask.delayed` to adapt `sklearn.datasets.make_blobs`, so that the actual dataset is being generated on our workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_block = 200000\n",
    "n_blocks = 500\n",
    "\n",
    "delayeds = [dask.delayed(make_blobs)(n_samples=n_samples_per_block,\n",
    "                                     centers=centers,\n",
    "                                     n_features=n_features,\n",
    "                                     random_state=i)[0]\n",
    "            for i in range(n_blocks)]\n",
    "arrays = [da.from_delayed(obj, shape=(n_samples_per_block, n_features), dtype=X.dtype)\n",
    "          for obj in delayeds]\n",
    "X = da.concatenate(arrays)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.persist()  # Only run this on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms implemented in Dask-ML are scalable. They handle larger-than-memory datasets just fine.\n",
    "\n",
    "They follow the scikit-learn API, so if you're familiar with scikit-learn, you'll feel at home with Dask-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_[:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap (for this entire notebook!):**\n",
    "* broke down machine learning scaling problems into to two categories (data size vs. model size).\n",
    "* solved an ML problem with a single machine (with Scikit-Learn).\n",
    "* solved an ML problem with a single machine and parallelism (with Scikit-Learn and Joblib).\n",
    "* solved an ML problem with a multiple machines and parallelism (with Scikit-Learn, Joblib and Dask).\n",
    "* solved an ML problem with a multiple machines *in the cloud* and parallelism (with Scikit-Learn, Joblib, Dask and Coiled).\n",
    "\n",
    "We also\n",
    "*  used `dask-ml` estimators that work well on Dask Arrays and DataFrames to train on datasets larger than your machine's RAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
